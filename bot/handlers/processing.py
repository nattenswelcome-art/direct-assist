from aiogram import Router, F, types
from aiogram.filters import Command
from aiogram.fsm.context import FSMContext
from bot.states import BotStates
from bot.keyboards.main_kb import get_main_kb
from services.yandex_api import yandex_service
from services.openai_service import openai_service
from services.sheets_service import sheets_service
from utils.logger import get_logger

logger = get_logger("handlers")
router = Router()

@router.message(Command("start"))
async def cmd_start(message: types.Message, state: FSMContext):
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! –Ø AI-–º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥.\n"
        "–Ø —É–º–µ—é —Å–æ–±–∏—Ä–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏–∫—É –∏–∑ Wordstat, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å –µ—ë –∏ –ø–∏—Å–∞—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è.\n"
        "–ù–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ –º–∞—Å–∫—É –∑–∞–ø—Ä–æ—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–∫—É–ø–∏—Ç—å —Å–ª–æ–Ω–∞').",
        reply_markup=get_main_kb()
    )
    await state.set_state(BotStates.waiting_for_keyword)

@router.message(F.text == "–°–æ–±—Ä–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏–∫—É")
async def btn_collect(message: types.Message, state: FSMContext):
    await message.answer("–í–≤–µ–¥–∏—Ç–µ –±–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∞—Å–∫—É), –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –±—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å Wordstat:")
    await state.set_state(BotStates.waiting_for_keyword)

@router.message(F.text == "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞")
async def btn_manual(message: types.Message, state: FSMContext):
    await message.answer("–ü—Ä–∏—à–ª–∏—Ç–µ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏), –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è:")
    await state.set_state(BotStates.waiting_for_list)

@router.message(BotStates.waiting_for_list)
async def process_manual_list(message: types.Message, state: FSMContext):
    raw_text = message.text
    if not raw_text: return
    
    # Split by lines and clean
    phrases = [line.strip() for line in raw_text.split('\n') if line.strip()]
    
    if not phrases:
        await message.answer("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç.")
        return

    # Mock semantics format: (phrase, 0) since we don't have stats
    semantics = [(p, 0) for p in phrases]
    seed_word = "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
    
    await run_pipeline(message, state, semantics, seed_word)

async def run_pipeline(message: types.Message, state: FSMContext, semantics: list, seed_word: str):
    """Reusable pipeline logic"""
    status_msg = await message.answer(f"‚úÖ –ü—Ä–∏–Ω—è—Ç–æ {len(semantics)} —Ñ—Ä–∞–∑.\nüß† –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞...")
    
    # Just list of strings for clustering
    phrases = [s[0] for s in semantics]
    phrase_map = {s[0]: s[1] for s in semantics} # Map back to shows
    
    # 2. Cluster
    try:
        clusters = await openai_service.cluster_keywords(phrases)
    except Exception as e:
        logger.error(f"Cluster fail: {e}")
        await status_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (OpenAI).")
        return
    
    await status_msg.edit_text(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–æ –Ω–∞ {len(clusters)} –≥—Ä—É–ø–ø.\n‚úçÔ∏è –ù–∞–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
    
    # 3. Generate Ads & Prepare Data
    report_data = {}
    
    # Process each cluster
    total_clusters = len(clusters)
    for i, (group_name, group_keywords) in enumerate(clusters.items()):
        # Update progress every 3 clusters
        if i % 3 == 0:
             await status_msg.edit_text(f"‚úçÔ∏è –ü–∏—à—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è: –ì—Ä—É–ø–ø–∞ {i+1}/{total_clusters}...")
        
        ads = await openai_service.generate_ads(group_name, group_keywords)
        
        # Format for sheet
        group_data_w_stats = []
        for kw in group_keywords:
            group_data_w_stats.append((kw, phrase_map.get(kw, 0)))
            
        report_data[group_name] = {
            "ads": ads,
            "keywords": group_data_w_stats
        }
    
    await status_msg.edit_text("‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏—è –≥–æ—Ç–æ–≤—ã.\nüìä –°–æ–∑–¥–∞—é Google –¢–∞–±–ª–∏—Ü—É...")
    
    # 4. Export to Sheets
    try:
        url = await sheets_service.create_report_sheet(message.from_user.id, seed_word, report_data)
        if url:
             await status_msg.edit_text(f"üéâ –ì–æ—Ç–æ–≤–æ! –í–∞—à –æ—Ç—á–µ—Ç:\n{url}")
        else:
             await status_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞).")
    except Exception as e:
        logger.error(f"Sheet error: {e}")
        await status_msg.edit_text("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ.")
        
    await state.set_state(BotStates.waiting_for_keyword)

@router.message(BotStates.waiting_for_keyword)
async def process_keyword(message: types.Message, state: FSMContext):
    keyword = message.text
    if not keyword:
        return
        
    status_msg = await message.answer(f"üöÄ –ù–∞—á–∏–Ω–∞—é —Ä–∞–±–æ—Ç—É –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{keyword}'...\n‚è≥ –°–±–æ—Ä —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏–∑ Wordstat...")
    
    # 1. Collect Semantics
    try:
        semantics = await yandex_service.collect_semantics(keyword)
    except Exception as e:
        logger.error(f"Error collecting semantics: {e}")
        await status_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å Yandex API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏.")
        return

    if not semantics:
        await status_msg.edit_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (–∏–ª–∏ –ø—É—Å—Ç–æ, –∏–ª–∏ –æ—à–∏–±–∫–∞ API).")
        return

    # HACK: delete status_msg to prevent confusion, rely on new pipeline msg
    await status_msg.delete()
    
    await run_pipeline(message, state, semantics, keyword)
